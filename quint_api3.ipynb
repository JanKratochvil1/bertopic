{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Berthopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"rogan.txt\", \"r\")\n",
    "text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n"
     ]
    }
   ],
   "source": [
    "sentences_clean = []\n",
    "counter = 0\n",
    "for item in sentences:\n",
    "    sentences_clean.append(clean(sentences[counter]))\n",
    "    counter = counter + 1\n",
    "\n",
    "counter = 0\n",
    "for item in sentences_clean:\n",
    "    sentences_clean[counter] = ' '.join(sentences_clean[counter])\n",
    "    counter = counter + 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1359/1359 [00:28<00:00, 47.53it/s]\n",
      "2022-09-07 14:20:37,220 - BERTopic - Transformed documents to Embeddings\n",
      "2022-09-07 14:20:40,251 - BERTopic - Reduced dimensionality\n",
      "2022-09-07 14:20:40,292 - BERTopic - Clustered reduced embeddings\n",
      "2022-09-07 14:20:41,331 - BERTopic - Reduced number of topics from 2 to 2\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "\n",
    "hf_model = pipeline(\"feature-extraction\", model=\"distilbert-base-cased\")\n",
    "\n",
    "topic_model = BERTopic()\n",
    "\n",
    "topic_model = BERTopic(language=\"english\", embedding_model=hf_model,calculate_probabilities=True, verbose=True, n_gram_range=(1, 3), nr_topics=\"auto\")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(sentences_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jan_kratochvil/code/JanKratochvil1/Final_Project/Bertopic_mynotebook/fastpai/quint_api3.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jan_kratochvil/code/JanKratochvil1/Final_Project/Bertopic_mynotebook/fastpai/quint_api3.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m probs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          -1_people_america_book_think\n",
       "1                 0_ayahuasca_cannabis_dmt_psychedelics\n",
       "2     1_archaeologist_natural history museum_san_arc...\n",
       "3                 2_authority_internet_fear_nationalism\n",
       "4     3_rainforest_evidence_bean farm_million square...\n",
       "5                4_beautiful_need_done_looking hard wow\n",
       "6     5_suddenly oh_oh applause_suddenly oh applause...\n",
       "7                          6_idea_debate_matter_one day\n",
       "8     7_civilization_old world_advanced civilization...\n",
       "9     8_serpent mound_summer solstice_head_natural r...\n",
       "10    9_neanderthal_denisovans_anatomically modern h...\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()['Name'][0:11]\n",
    "#topics['Name']\n",
    "#df['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(topics) > 2:\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_be_deleted = []\n",
    "for i in range(2,len(topics)-1):\n",
    "        words_to_be_deleted.append(topic_model.get_topic(i)[0][0].split(' ')[0])\n",
    "words_to_be_deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "def clean2(text):\n",
    "    tokenized = word_tokenize(text) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = words_to_be_deleted # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    return without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n"
     ]
    }
   ],
   "source": [
    "sentences_clean2 = []\n",
    "counter = 0\n",
    "for item in sentences_clean:\n",
    "    sentences_clean2.append(clean2(item))\n",
    "    counter = counter + 1\n",
    "\n",
    "counter = 0\n",
    "for item in sentences_clean2:\n",
    "    sentences_clean2[counter] = ' '.join(item)\n",
    "    counter = counter + 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Bertopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "topic_model = BERTopic(embedding_model=sentence_model, language='english', diversity = 0.4, top_n_words=10, n_gram_range=(1, 3), min_topic_size=6)\n",
    "topics, probabilities = topic_model.fit_transform(sentences_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1359"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0_ayahuasca_cannabis_dmt_psychedelics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1_archaeologist_natural history museum_san_arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>2_authority_internet_fear_nationalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>3_rainforest_evidence_bean farm_million square...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4_beautiful_need_done_looking hard wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>5_suddenly oh_oh applause_suddenly oh applause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>6_idea_debate_matter_one day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>7_civilization_old world_advanced civilization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>8_serpent mound_summer solstice_head_natural r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>9_neanderthal_denisovans_anatomically modern h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "1       0     71              0_ayahuasca_cannabis_dmt_psychedelics\n",
       "2       1     51  1_archaeologist_natural history museum_san_arc...\n",
       "3       2     44              2_authority_internet_fear_nationalism\n",
       "4       3     35  3_rainforest_evidence_bean farm_million square...\n",
       "5       4     30             4_beautiful_need_done_looking hard wow\n",
       "6       5     29  5_suddenly oh_oh applause_suddenly oh applause...\n",
       "7       6     28                       6_idea_debate_matter_one day\n",
       "8       7     28  7_civilization_old world_advanced civilization...\n",
       "9       8     26  8_serpent mound_summer solstice_head_natural r...\n",
       "10      9     24  9_neanderthal_denisovans_anatomically modern h..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = topic_model.get_topic_info()\n",
    "topics.iloc[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "if len(topics) > 3:\n",
    "    print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authority',\n",
       " 'rainforest',\n",
       " 'beautiful',\n",
       " 'suddenly oh',\n",
       " 'idea',\n",
       " 'civilization',\n",
       " 'serpent mound',\n",
       " 'neanderthal',\n",
       " 'dna',\n",
       " 'blue stone',\n",
       " 'know called',\n",
       " 'orion',\n",
       " 'ice age',\n",
       " 'hundred year',\n",
       " 'torrid meteor stream',\n",
       " 'mayan',\n",
       " 'bimini road',\n",
       " 'map',\n",
       " 'burn',\n",
       " 'meme',\n",
       " 'solid information',\n",
       " 'talk event',\n",
       " 'back image',\n",
       " 'clovis first',\n",
       " 'consciousness',\n",
       " 'specie',\n",
       " 'civilization',\n",
       " 'well say mean',\n",
       " 'barnes noble',\n",
       " 'scale',\n",
       " 'rainforest',\n",
       " 'yeah really still',\n",
       " 'music',\n",
       " 'america',\n",
       " 'tepe',\n",
       " 'published nature',\n",
       " 'circle',\n",
       " 'gatherer',\n",
       " 'air burst',\n",
       " 'cosmic impact',\n",
       " 'feathered serpent',\n",
       " 'sort advanced',\n",
       " 'stephen',\n",
       " 'population',\n",
       " 'conclusion',\n",
       " 'journey',\n",
       " 'technological feat longitude',\n",
       " 'humongous crater',\n",
       " 'running middle',\n",
       " 'interview earn even',\n",
       " 'tech look',\n",
       " 'physical',\n",
       " 'time lot like',\n",
       " 'go find',\n",
       " 'nothing reason']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_be_deleted = []\n",
    "if len(topics) > 3:\n",
    "    for k in range(2,len(topics)-1):\n",
    "        words_to_be_deleted.append(topic_model.get_topic(k)[0][0])\n",
    "words_to_be_deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural history museum'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(1)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ayahuasca'],\n",
       " ['archaeologist'],\n",
       " ['authority'],\n",
       " ['rainforest'],\n",
       " ['beautiful'],\n",
       " ['suddenly oh'],\n",
       " ['idea'],\n",
       " ['civilization'],\n",
       " ['serpent mound'],\n",
       " ['neanderthal']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "list = []\n",
    "for k in range(10):\n",
    "    list_temp = []\n",
    "    for i in range(1):\n",
    "        list_temp.append(topic_model.get_topic(k)[i][0])\n",
    "    list.append(list_temp)\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee5da7848762d62e8dbebda05db5c3c89b8fe380c07223bbbe6d30d386ff9d0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
